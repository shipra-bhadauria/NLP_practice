{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Summarization",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOffFz2WtphP",
        "outputId": "0e361f1f-7b66-4ab4-b8f2-72025c271dcb"
      },
      "source": [
        "pip install beautifulsoup4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xo1-rqkMtvIZ",
        "outputId": "14c1ada4-ae22-4784-9323-6514c6b74cdb"
      },
      "source": [
        "pip install lxml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (4.2.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_tL5UJ0t1eV"
      },
      "source": [
        "import bs4 as bs\r\n",
        "import urllib.request\r\n",
        "import re\r\n",
        "\r\n",
        "scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Artificial_intelligence')\r\n",
        "article  = scraped_data.read()\r\n",
        "parsed_article = bs.BeautifulSoup(article, 'lxml')\r\n",
        "paragraphs = parsed_article.find_all('p')\r\n",
        "article_text = \"\"\r\n",
        "for p in paragraphs:\r\n",
        "  article_text += p.text"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaDAN02st4kH"
      },
      "source": [
        "# Removing Square Brackets and Extra Spaces\r\n",
        "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\r\n",
        "article_text = re.sub(r'\\s+', ' ', article_text)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuOmXBGCuBKb"
      },
      "source": [
        "# Removing special characters and digits\r\n",
        "formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )\r\n",
        "formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbZc4lH4uG4N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd5f903c-5875-48c9-d973-f2699c5d04a5"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "sentence_list = nltk.sent_tokenize(article_text)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMVMpvkZuIpN",
        "outputId": "12455bca-a4e9-499e-fb4f-744d0a1060ee"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "stopwords = nltk.corpus.stopwords.words('english')\r\n",
        "\r\n",
        "word_frequencies = {}\r\n",
        "for word in nltk.word_tokenize(formatted_article_text):\r\n",
        "    if word not in stopwords:\r\n",
        "        if word not in word_frequencies.keys():\r\n",
        "            word_frequencies[word] = 1\r\n",
        "        else:\r\n",
        "            word_frequencies[word] += 1"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foCqBehkuwnV"
      },
      "source": [
        "maximum_frequncy = max(word_frequencies.values())\r\n",
        "for word in word_frequencies.keys():\r\n",
        "  word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0aHixQhu7uq"
      },
      "source": [
        "sentence_scores = {}\r\n",
        "for sent in sentence_list:\r\n",
        "    for word in nltk.word_tokenize(sent.lower()):\r\n",
        "        if word in word_frequencies.keys():\r\n",
        "            if len(sent.split(' ')) < 30:\r\n",
        "                if sent not in sentence_scores.keys():\r\n",
        "                    sentence_scores[sent] = word_frequencies[word]\r\n",
        "                else:\r\n",
        "                    sentence_scores[sent] += word_frequencies[word]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRd69O8FvBpe",
        "outputId": "e91554a2-d31a-4454-f1ff-3777fa143385"
      },
      "source": [
        "import heapq\r\n",
        "summary_sentences = heapq.nlargest(7, sentence_scores, key = sentence_scores.get)\r\n",
        "\r\n",
        "summary = ' '.join(summary_sentences)\r\n",
        "print(summary)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Artificial intelligence (AI), is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals, which involves consciousness and emotionality. 'Strong' AI is usually labelled as AGI (Artificial General Intelligence) while attempts to emulate 'natural' intelligence have been called ABI (Artificial Biological Intelligence). Musk also funds companies developing artificial intelligence such as DeepMind and Vicarious to \"just keep an eye on what's going on with artificial intelligence. A superintelligence, hyperintelligence, or superhuman intelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. A February 2020 European Union white paper on artificial intelligence advocated for artificial intelligence for economic benefits, including \"improving healthcare (e.g. Many of the problems in this article may also require general intelligence, if machines are to solve the problems as well as people do. The overall research goal of artificial intelligence is to create technology that allows computers and machines to function in an intelligent manner.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}